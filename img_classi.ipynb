{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9236f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Using device: cpu\n",
      "‚úÖ Classes: ['cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚öôÔ∏è Using device: {device}\")\n",
    "\n",
    "# Transform: resize ‚Üí tensor ‚Üí normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "train_ds = datasets.ImageFolder(root='cifar2/train', transform=transform)\n",
    "valid_ds = datasets.ImageFolder(root='cifar2/valid', transform=transform)\n",
    "class_names = train_ds.classes \n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "# Print class names\n",
    "print(f\"‚úÖ Classes: {train_ds.classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc09ea6",
   "metadata": {},
   "source": [
    "mobilenetv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba0c6554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viju1\\Desktop\\img classification\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\viju1\\Desktop\\img classification\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\viju1/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 complete.\n",
      "‚úÖ Epoch 2 complete.\n",
      "‚úÖ Epoch 3 complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transforms (smaller image size)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "train_ds = datasets.ImageFolder(\"cifar2/train\", transform=transform)\n",
    "valid_ds = datasets.ImageFolder(\"cifar2/valid\", transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# Model\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "for param in mobilenet.parameters():\n",
    "    param.requires_grad = False  # freeze backbone\n",
    "mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mobilenet.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "mobilenet.train()\n",
    "for epoch in range(3):  # quick training\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"‚úÖ Epoch {epoch+1} complete.\")\n",
    "\n",
    "# Save model\n",
    "torch.save(mobilenet.state_dict(), \"mobilenetv2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc716396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MobileNetV2 Accuracy: 86.00%\n",
      "‚ö° Avg Inference Time: 19.21 ms/image\n"
     ]
    }
   ],
   "source": [
    "mobilenet.eval()\n",
    "y_true, y_pred = [], []\n",
    "inference_times = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        start = time.time()\n",
    "        outputs = mobilenet(images)\n",
    "        end = time.time()\n",
    "\n",
    "        preds = outputs.argmax(dim=1).cpu().item()\n",
    "        y_pred.append(preds)\n",
    "        y_true.append(labels.item())\n",
    "        inference_times.append((end - start) * 1000)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "avg_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "print(f\"üéØ MobileNetV2 Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"‚ö° Avg Inference Time: {avg_time:.2f} ms/image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef34aaf",
   "metadata": {},
   "source": [
    "MobileNetV3Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23572d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\viju1\\Desktop\\img classification\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\viju1\\Desktop\\img classification\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to C:\\Users\\viju1/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 done.\n",
      "‚úÖ Epoch 2 done.\n",
      "‚úÖ Epoch 3 done.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import mobilenet_v3_small\n",
    "\n",
    "# Model\n",
    "mobilenetv3 = mobilenet_v3_small(pretrained=True)\n",
    "for param in mobilenetv3.parameters():\n",
    "    param.requires_grad = False  # freeze backbone\n",
    "mobilenetv3.classifier[3] = nn.Linear(mobilenetv3.classifier[3].in_features, 2)\n",
    "mobilenetv3 = mobilenetv3.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(mobilenetv3.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "mobilenetv3.train()\n",
    "for epoch in range(3):  # minimal epochs\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mobilenetv3(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"‚úÖ Epoch {epoch+1} done.\")\n",
    "\n",
    "# Save model\n",
    "torch.save(mobilenetv3.state_dict(), \"mobilenetv3small.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43e37308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MobileNetV3Small Accuracy: 76.00%\n",
      "‚ö° Avg Inference Time: 11.13 ms/image\n"
     ]
    }
   ],
   "source": [
    "mobilenetv3.eval()\n",
    "y_true, y_pred = [], []\n",
    "inference_times = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        start = time.time()\n",
    "        outputs = mobilenetv3(images)\n",
    "        end = time.time()\n",
    "\n",
    "        preds = outputs.argmax(dim=1).cpu().item()\n",
    "        y_pred.append(preds)\n",
    "        y_true.append(labels.item())\n",
    "        inference_times.append((end - start) * 1000)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "avg_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "print(f\"üéØ MobileNetV3Small Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"‚ö° Avg Inference Time: {avg_time:.2f} ms/image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f466038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.174  Python-3.13.5 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=cifar2, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=96, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_cls_962, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\classify\\yolov8n_cls_962, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\viju1\\Desktop\\img classification\\cifar2\\train... found 100 images in 2 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\viju1\\Desktop\\img classification\\cifar2\\valid... found 100 images in 2 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    332802  ultralytics.nn.modules.head.Classify         [256, 2]                      \n",
      "YOLOv8n-cls summary: 56 layers, 1,440,850 parameters, 1,440,850 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 140.844.5 MB/s, size: 14.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\viju1\\Desktop\\img classification\\cifar2\\train... 100 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 186.7104.3 MB/s, size: 18.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\viju1\\Desktop\\img classification\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\viju1\\Desktop\\img classification\\cifar2\\valid... 100 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 96 train, 96 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\yolov8n_cls_962\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\viju1\\Desktop\\img classification\\myenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "        1/5         0G     0.7843          4         96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  5.77it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.56          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.6516          4         96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  6.88it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.66          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.5845          4         96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  6.51it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.71          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G     0.5216          4         96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  9.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.73          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G     0.5176          4         96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  8.76it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.73          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.002 hours.\n",
      "Optimizer stripped from runs\\classify\\yolov8n_cls_962\\weights\\last.pt, 3.0MB\n",
      "Optimizer stripped from runs\\classify\\yolov8n_cls_962\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\yolov8n_cls_962\\weights\\best.pt...\n",
      "Ultralytics 8.3.174  Python-3.13.5 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,437,442 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\viju1\\Desktop\\img classification\\cifar2\\train... found 100 images in 2 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\viju1\\Desktop\\img classification\\cifar2\\valid... found 100 images in 2 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.73          1\n",
      "Speed: 0.0ms preprocess, 1.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\yolov8n_cls_962\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8n classifier\n",
    "yolo_model = YOLO('yolov8n-cls.pt')\n",
    "\n",
    "# Train on your dataset with smaller image size\n",
    "results = yolo_model.train(\n",
    "    data='cifar2',      # path to dataset\n",
    "    epochs=5,           \n",
    "    imgsz=96,           # small image size for speed\n",
    "    batch=16,\n",
    "    name='yolov8n_cls_96'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcb8d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n-cls summary (fused): 30 layers, 1,437,442 parameters, 0 gradients, 3.3 GFLOPs\n",
      "üéØ YOLOv8n Accuracy: 73.00%\n",
      "‚ö° Avg Inference Time: 8.60 ms/image\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Load best weights\n",
    "yolo_loaded = YOLO('runs/classify/yolov8n_cls_96/weights/best.pt')\n",
    "yolo_loaded.fuse()\n",
    "\n",
    "# Collect test images\n",
    "valid_path = 'cifar2/valid'\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for class_index, class_name in enumerate(sorted(os.listdir(valid_path))):\n",
    "    class_dir = os.path.join(valid_path, class_name)\n",
    "    for fname in os.listdir(class_dir):\n",
    "        if fname.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            image_paths.append(os.path.join(class_dir, fname))\n",
    "            labels.append(class_index)\n",
    "\n",
    "# Inference\n",
    "y_true, y_pred, inference_times = [], [], []\n",
    "\n",
    "for path, label in zip(image_paths, labels):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    \n",
    "    start = time.time()\n",
    "    results = yolo_loaded(img, verbose=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    pred = int(results[0].probs.top1)\n",
    "    y_true.append(label)\n",
    "    y_pred.append(pred)\n",
    "    inference_times.append((end - start) * 1000)\n",
    "\n",
    "# Accuracy & time\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "avg_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "print(f\"üéØ YOLOv8n Accuracy: {acc * 100:.2f}%\")\n",
    "print(f\"‚ö° Avg Inference Time: {avg_time:.2f} ms/image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3269a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
